{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0bf2027-8945-4737-b356-c3992147baf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "High Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf64bc4-db45-4786-89a5-b858e6b32752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required Excel reader\n",
    "%pip install openpyxl\n",
    "\n",
    "# Import libraries\n",
    "from boxsdk import JWTAuth, Client\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql.functions import col, trim, split, size, when\n",
    "\n",
    "# Authenticate with Box using secrets from Databricks\n",
    "auth = JWTAuth(\n",
    "    client_id=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_client_id\"),\n",
    "    client_secret=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_client_secret\"),\n",
    "    enterprise_id=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_enterprise_id\"),\n",
    "    jwt_key_id=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_jwt_key_id\"),\n",
    "    rsa_private_key_data=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"rsa_private_key_data\"),\n",
    "    rsa_private_key_passphrase=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_private_key_pass_phrase\")\n",
    ")\n",
    "\n",
    "client = Client(auth)\n",
    "\n",
    "# Download Excel file from Box\n",
    "file_id = 'your_file_id'\n",
    "file_stream = BytesIO(client.file(file_id).content())\n",
    "df = pd.read_excel(file_stream)\n",
    "\n",
    "# ✅ Fix Governance Date parsing\n",
    "if 'Governance Date' in df.columns:\n",
    "    df['Governance Date'] = pd.to_datetime(df['Governance Date'], errors='coerce')\n",
    "\n",
    "# Clean column names\n",
    "df.columns = [re.sub(r'\\W+', '_', col).strip('_') for col in df.columns]\n",
    "df.replace(\"NULL\", pd.NA, inplace=True)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Basic cleanup\n",
    "spark_df = spark_df.withColumn(\"Total_Contract_Cost\", col(\"Total_Contract_Cost\").cast(\"double\"))\n",
    "for column in spark_df.columns:\n",
    "    if dict(spark_df.dtypes)[column] == 'string':\n",
    "        spark_df = spark_df.withColumn(column, trim(col(column)))\n",
    "if \"Contract_Lead\" in spark_df.columns:\n",
    "    spark_df = spark_df.withColumn(\"Contract_Lead\", split(col(\"Contract_Lead\"), \",\"))\n",
    "\n",
    "# Extract Vendor and Description from Contract_Name\n",
    "if \"Contract_Name\" in spark_df.columns:\n",
    "    parts = split(col(\"Contract_Name\"), \"\\|\")\n",
    "    spark_df = spark_df \\\n",
    "        .withColumn(\"Vendor\", when(size(parts) >= 1, trim(parts.getItem(0)))) \\\n",
    "        .withColumn(\"Description\", when(size(parts) >= 2, trim(parts.getItem(1))))\n",
    "\n",
    "# Save to Delta\n",
    "spark_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"development.team_game_boe.game_high_cost_active\")\n",
    "\n",
    "# ✅ Display contents of the table\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Vendor, Description, Nike_Contract_ID, Governance_Required, Governance_Date, Governance, Comments\n",
    "    FROM development.team_game_boe.game_high_cost_active\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8974dec2-75f6-46c2-85ec-a8020c11b7f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1 Milion +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20f25b8-3e75-448b-8358-ddae89a0ddfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from boxsdk import JWTAuth, Client\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql.functions import col, trim, split, size, when\n",
    "\n",
    "# Authenticate and download Excel file from Box\n",
    "auth = JWTAuth(\n",
    "    client_id=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_client_id\"),\n",
    "    client_secret=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_client_secret\"),\n",
    "    enterprise_id=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_enterprise_id\"),\n",
    "    jwt_key_id=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_jwt_key_id\"),\n",
    "    rsa_private_key_data=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"rsa_private_key_data\"),\n",
    "    rsa_private_key_passphrase=dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"box_private_key_pass_phrase\")\n",
    ")\n",
    "client = Client(auth)\n",
    "file_stream = BytesIO(client.file('your_file_id').content())\n",
    "df = pd.read_excel(file_stream)\n",
    "\n",
    "# ✅ Fix Governance Date parsing\n",
    "if 'Governance Date' in df.columns:\n",
    "    df['Governance Date'] = pd.to_datetime(df['Governance Date'], errors='coerce')\n",
    "\n",
    "# Clean column names\n",
    "df.columns = [re.sub(r'\\W+', '_', col).strip('_') for col in df.columns]\n",
    "df.replace(\"NULL\", pd.NA, inplace=True)\n",
    "\n",
    "# Ensure all columns are string type before creating Spark DataFrame\n",
    "df = df.astype(str)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Basic cleanup\n",
    "spark_df = spark_df.withColumn(\"Total_Contract_Cost\", col(\"Total_Contract_Cost\").cast(\"double\"))\n",
    "for column in spark_df.columns:\n",
    "    if dict(spark_df.dtypes)[column] == 'string':\n",
    "        spark_df = spark_df.withColumn(column, trim(col(column)))\n",
    "if \"Contract_Lead\" in spark_df.columns:\n",
    "    spark_df = spark_df.withColumn(\"Contract_Lead\", split(col(\"Contract_Lead\"), \",\"))\n",
    "\n",
    "# Extract Vendor and Description from Contract_Name\n",
    "if \"Contract_Name\" in spark_df.columns:\n",
    "    parts = split(col(\"Contract_Name\"), \"\\|\")\n",
    "    spark_df = spark_df \\\n",
    "        .withColumn(\"Vendor\", when(size(parts) >= 1, trim(parts.getItem(0)))) \\\n",
    "        .withColumn(\"Description\", when(size(parts) >= 2, trim(parts.getItem(1))))\n",
    "\n",
    "# Save to Delta\n",
    "spark_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"development.team_game_boe.game_1m_cost_active\")\n",
    "\n",
    "# ✅ Display contents of the table\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Vendor, Description, Nike_Contract_ID, Governance_Required, Governance_Date, Governance, Comments\n",
    "    FROM development.team_game_boe.game_1m_cost_active\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58c521a-9aef-4f02-906e-b95069b6f8b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Airtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa6df45-d01d-4e91-b2b6-9c32176fb3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ===== Install dependencies =====\n",
    "%pip install --upgrade typing_extensions pyairtable\n",
    "%pip install xlrd==2.0.1 openpyxl\n",
    "\n",
    "# ===== Imports =====\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp, expr\n",
    "import requests\n",
    "import datetime as dt\n",
    "\n",
    "# ===== Secrets =====\n",
    "airtable_access_token = dbutils.secrets.get(scope=\"game_boe_secrets\", key=\"airtable_access_token\")\n",
    "\n",
    "# ===== Airtable Config =====\n",
    "base_id = 'your_base_id'\n",
    "contracts_table_id = 'your_table_id'\n",
    "\n",
    "# ===== Functions =====\n",
    "def fetch_airtable_data(api_key, base_id, table_id):\n",
    "    \"\"\"Fetch all rows from Airtable table into pandas DataFrame.\"\"\"\n",
    "    url = f\"https://api.airtable.com/v0/{base_id}/{table_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    all_records, offset = [], None\n",
    "    while True:\n",
    "        params = {}\n",
    "        if offset:\n",
    "            params['offset'] = offset\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Error fetching data from Airtable: {response.text}\")\n",
    "        data = response.json()\n",
    "        records = data.get('records', [])\n",
    "        all_records.extend([r.get('fields', {}) for r in records])\n",
    "        offset = data.get('offset')\n",
    "        if not offset:\n",
    "            break\n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "def prepare_data(df, new_columns, date_columns):\n",
    "    \"\"\"Rename columns and format date columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\\\n",
    "                         .fillna(pd.Timestamp('1900-01-01'))\\\n",
    "                         .dt.strftime('%Y-%m-%d')\n",
    "    return df\n",
    "\n",
    "def standardize_cols(df):\n",
    "    \"\"\"Make column names Spark-safe.\"\"\"\n",
    "    cols = (pd.Series(df.columns)\n",
    "              .str.replace(' ', '_', regex=False)\n",
    "              .str.replace('(', '', regex=False)\n",
    "              .str.replace(')', '', regex=False)\n",
    "              .str.replace('/', '_', regex=False)\n",
    "              .str.replace('-', '_', regex=False)\n",
    "              .str.replace(r'[^A-Za-z0-9_]', '', regex=True))\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "# ===== Pull Airtable Data =====\n",
    "contracts_df = fetch_airtable_data(airtable_access_token, base_id, contracts_table_id)\n",
    "\n",
    "# ===== Prepare Data =====\n",
    "# Customize these mappings based on your Airtable schema\n",
    "contracts_df = prepare_data(contracts_df,\n",
    "    {'Example Column': 'Renamed Column'},  # Replace with actual mappings\n",
    "    ['Date Column 1', 'Date Column 2']     # Replace with actual date columns\n",
    ")\n",
    "\n",
    "contracts_df = standardize_cols(contracts_df)\n",
    "\n",
    "# ===== Convert to Spark & Write Delta =====\n",
    "contracts_sdf = spark.createDataFrame(contracts_df)\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "contracts_sdf = contracts_sdf.withColumn(\"upload_timestamp\", current_timestamp()) \\\n",
    "                             .withColumn(\"upload_timestamp\", expr(\"from_utc_timestamp(upload_timestamp, 'America/Los_Angeles')\"))\n",
    "\n",
    "contracts_sdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"development.team_game_boe.airtable_contracts_bronze\")\n",
    "\n",
    "display(contracts_sdf)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "interim_contract_pulls_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
